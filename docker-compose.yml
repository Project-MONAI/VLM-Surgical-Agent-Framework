version: '3'

services:
  vllm:
    image: gitlab-master.nvidia.com:5005/holoscan/copilot-blueprint:vllm-openai-v0.8.3-dgpu-a6000
    command: >
      --model models/llm/Llama-3.2-11B-lora-surgical-4bit
      --enforce-eager
      --max-model-len 32000
      --max-num-seqs 8
      --disable-mm-preprocessor-cache
      --load-format bitsandbytes
      --quantization bitsandbytes
    volumes:
      - $HOME/nvidia/VLM-Surgical-Agent-Framework/models:/vllm-workspace/models
    network_mode: host
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/v1/models"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ["0"]
              capabilities: [gpu]

  whisper:
    image: gitlab-master.nvidia.com:5005/holoscan/copilot-blueprint:whisper-dgpu
    command: --model_cache_dir /root/whisper
    volumes:
      - $HOME/nvidia/VLM-Surgical-Agent-Framework/models/whisper:/root/whisper
    network_mode: host
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ["0"]
              capabilities: [gpu]

  ui:
    build:
      context: .
      dockerfile: docker/Dockerfile.ui
    image: gitlab-master.nvidia.com:5005/holoscan/copilot-blueprint:ui
    network_mode: host
    volumes:
      - $HOME/nvidia/VLM-Surgical-Agent-Framework/annotations:/app/annotations
    depends_on:
      vllm:
        condition: service_healthy
      whisper:
        condition: service_started
