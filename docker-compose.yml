version: '3'

services:
  vllm:
    image: vlm-surgical-agents:vllm-openai-v0.8.3-dgpu
    command: >
      --model models/llm/Llama-3.2-11B-lora-surgical-4bit
      --enforce-eager
      --max-model-len 32000
      --max-num-seqs 8
      --disable-mm-preprocessor-cache
      --load-format bitsandbytes
      --quantization bitsandbytes
    volumes:
      - ./models:/vllm-workspace/models
    network_mode: host
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/v1/models"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ["0"]
              capabilities: [gpu]

  whisper:
    image: vlm-surgical-agents:whisper-dgpu
    command: --model_cache_dir /root/whisper
    volumes:
      - ./models/whisper:/root/whisper
    network_mode: host
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ["0"]
              capabilities: [gpu]

  ui:
    build:
      context: .
      dockerfile: docker/Dockerfile.ui
    image: vlm-surgical-agents:ui
    network_mode: host
    depends_on:
      vllm:
        condition: service_healthy
      whisper:
        condition: service_started
